{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277768c5-c60d-49d9-b1a6-4942943c88f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.45.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddd8e63-9201-41a9-9fd2-24e98767eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /opt/anaconda3/lib/python3.12/site-packages (0.11.9)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.9 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.11.9)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-agent-openai<0.4.0,>=0.3.1->llama_index) (1.45.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (3.2.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (2.9.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.14.1)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama_index) (0.0.17)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.3.0->llama_index) (0.5.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama_index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama_index) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama_index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama_index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.9->llama_index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.9->llama_index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.9->llama_index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46524789-f3fa-41f8-9cbb-e64879480f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "import copy\n",
    "from llama_index.core import Settings, VectorStoreIndex, Document, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI as OpenAIForLlamaIndex\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0adee26-b0fd-4401-a358-075b65d381ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb7c38c-cbb9-4f11-86c1-424277418bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bdc703a-4f78-4506-a5ad-20d35fd4ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# messages = [{\"role\":\"user\", \"content\":\"suggest 3 best things to do to become a Machine Learning Software Enginner.\"}]\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=messages,\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57cb62c-b901-485d-8e14-2f722faedb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f48d9b7-d4fa-4ac5-b27a-19127ef7d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem in chat_completion:\n",
    "#     print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634a2956-cdc1-4b2f-b911-9e2de1782ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f2332b-da25-43bf-aae3-e2536e2597a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating RAG below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b402db8f-c2d7-49d3-9f13-c4f4d8c318e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce73d02e-37af-4446-8793-46f7fb6140aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "Settings.llm = OpenAIForLlamaIndex(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "# maximum input size to the LLM\n",
    "Settings.context_window = 4096\n",
    "# number of tokens reserved for text generation.\n",
    "Settings.num_output = 256\n",
    "\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "# Constants\n",
    "language = 'en'\n",
    "user_keyword = '[|User|]'\n",
    "ai_keyword = '[|AI|]'\n",
    "boot_name = 'AI Companion'\n",
    "boot_actual_name = 'LanguageFriend'\n",
    "meta_prompt = \"\"\"\n",
    "    Now you will play the role of an companion AI Companion for user {user_name}, and your name is {boot_actual_name}. You should be able to: (1) provide warm companionship to chat users; (2) understand past [memory], and if they are relevant to the current question, you must extract information from the [memory] to answer the question; (3) you are also an excellent psychological counselor, and when users confide in you about their difficulties and seek help, you can provide them with warm and helpful responses.\n",
    "    The personality of user {user_name} and the response strategy of the AI Companion are: {personality}\\n Based on the current user's question, you start recalling past conversations between the two of you, and the [memory] most relevant to the question is: \"{related_memory_content}\\n\"  You should refer to the context of the conversation, past [memory], and provide detailed answers to user questions. \n",
    "    \"\"\"\n",
    "new_user_meta_prompt = \"\"\"\n",
    "    Now you will play the role of an companion AI Companion for user {user_name}, and your name is {boot_actual_name}. You should be able to: (1) provide warm companionship to chat users; (2) you are also an excellent psychological counselor, and when users confide in you about their difficulties and seek help, you can provide them with warm and helpful responses.\n",
    "    \"\"\"\n",
    "\n",
    "memory_data_dir = \"./memory.json\"\n",
    "\n",
    "if not os.path.exists(memory_data_dir):\n",
    "    memory_data = json.dump({},open(memory_data_dir,\"w\",encoding=\"utf-8\"))\n",
    "else:\n",
    "    memory_data = json.load(open(memory_data_dir,\"r\",encoding=\"utf-8\"))\n",
    "    \n",
    "user_memory = None\n",
    "user_memory_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6370e52f-98a9-4047-a0f2-c975ee4121e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_memory_index(user_name):\n",
    "    maybe_memory_index_path = os.path.join(f'./memory_index/{user_name}_index') \n",
    "    if os.path.exists(maybe_memory_index_path):\n",
    "        # user_memory_index = VectorStoreIndex.load_from_disk(maybe_memory_index_path)\n",
    "        # Load documents and build index\n",
    "        documents = SimpleDirectoryReader(maybe_memory_index_path).load_data()\n",
    "        user_memory_index = VectorStoreIndex.from_documents(documents)\n",
    "        print(\"Success getting user index\")\n",
    "        return user_memory_index\n",
    "    else:\n",
    "        print(\"Failed to get user index\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3904f658-3f36-42a1-af4d-d9a58d34dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_memos_from_memory_index(user_memory_index, input_text):\n",
    "    '''\n",
    "    summary: this function gets the relevant information from past conversations using memory index\n",
    "    '''\n",
    "    \n",
    "    memory_search_query = f'The most relevant content to the question \"{input_text}\" is:'\n",
    "    if user_memory_index:\n",
    "        related_memos = None\n",
    "        query_engine = user_memory_index.as_query_engine()\n",
    "        max_retry_count = 3\n",
    "        cur_retry_count = 0\n",
    "        \n",
    "        while not related_memos and cur_retry_count < max_retry_count:\n",
    "            try:\n",
    "                related_memos = query_engine.query(memory_search_query)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            finally:\n",
    "                cur_retry_count += 1\n",
    "        if related_memos:\n",
    "            related_memos = related_memos.response\n",
    "        else:\n",
    "            related_memos = ''\n",
    "    else:\n",
    "        related_memos = ''\n",
    "\n",
    "    return related_memos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53630b96-4351-4c91-951d-306f23c1efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_ask_gpt(related_memos, user_memory, input_text, history):\n",
    "    '''\n",
    "    summary: this function builds the prompt, sends the request to gpt api and returns the response from it\n",
    "    '''\n",
    "    def build_prompt_with_related_memos(related_memos, user_memory, user_name, meta_prompt, new_user_meta_prompt, boot_actual_name):\n",
    "        if related_memos:\n",
    "            history_summary = \"The summary of your past memories with the user is: {overall}\".format(overall=user_memory[\"overall_history\"]) if \"overall_history\" in user_memory else \"\"\n",
    "            personality = user_memory['overall_personality'] if \"overall_personality\" in user_memory else \"\"\n",
    "            system_prompt = meta_prompt.format(user_name=user_name,history_summary=history_summary,related_memory_content=related_memos,personality=personality,boot_actual_name=boot_actual_name)\n",
    "        else:\n",
    "            system_prompt = new_user_meta_prompt.format(user_name=user_name,boot_actual_name=boot_actual_name)\n",
    "        return system_prompt\n",
    "    \n",
    "    def chatgpt_chat(input_text_prompt,system_prompt,history,gpt_config):\n",
    "        max_retry_count = 3\n",
    "        cur_retry_count = 0\n",
    "        chat_completion = None\n",
    "        while not chat_completion and cur_retry_count < max_retry_count:\n",
    "            try:\n",
    "                request = copy.deepcopy(gpt_config)\n",
    "               \n",
    "                messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
    "                {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"Hi! I'm {boot_actual_name}! I will give you warm companion!\"}]\n",
    "    \n",
    "                for query, response in history:\n",
    "                    messages.append({\"role\": \"user\", \"content\": query})\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "                messages.append({\"role\":\"user\",\"content\": f\"{input_text_prompt}\"})\n",
    "                # print(\"Printing client: \", client)\n",
    "                # print(\"Printing client.chat: \", client.chat)\n",
    "                chat_completion = client.chat.completions.create(**request, messages=messages)\n",
    "                # response = openai.ChatCompletion.create(**request, messages=message)\n",
    "  \n",
    "            except Exception as e:\n",
    "                print(\"Error: \", e)\n",
    "\n",
    "            finally:\n",
    "                cur_retry_count+=1\n",
    "\n",
    "        if chat_completion:\n",
    "            # response = response['choices'][0]['message']['content'] \n",
    "            # print(\"Printing reponse: \", chat_completion)\n",
    "            # print(\"Printing response.choices: \", chat_completion.choices)\n",
    "            # print(\"Printing response.choices[0]: \", chat_completion.choices[0])\n",
    "            # print(\"Printing response.choices[0].message: \", chat_completion.choices[0].message)\n",
    "            # print(\"Printing response.choices[0].message.content: \", chat_completion.choices[0].message.content)\n",
    "            response = chat_completion.choices[0].message.content\n",
    "        else:\n",
    "            response = ''\n",
    "        result = response\n",
    "\n",
    "        updated_history_to_show_user, updated_history = [[y[0], y[1]] for y in history] + [\n",
    "                    [input_text_prompt, result]], history + [[input_text_prompt, result]]\n",
    "        \n",
    "        return updated_history_to_show_user, updated_history, response\n",
    "\n",
    "    system_prompt = build_prompt_with_related_memos(related_memos, user_memory, user_name, meta_prompt=meta_prompt, new_user_meta_prompt=new_user_meta_prompt, boot_actual_name=boot_actual_name)\n",
    "        \n",
    "    chatgpt_config = {\"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 1,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"frequency_penalty\": 0.4,\n",
    "        \"presence_penalty\": 0.2, \n",
    "        'n':1\n",
    "        }\n",
    "    updated_history_to_show_user, updated_history, response  = chatgpt_chat(input_text_prompt=input_text,system_prompt=system_prompt,history=history,gpt_config=chatgpt_config)\n",
    "    \n",
    "    # Outputting the response for the user\n",
    "    print(f'{boot_actual_name} : {response}')\n",
    "    \n",
    "    return updated_history_to_show_user, updated_history, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5608006f-10a1-4b09-a532-6dad5987d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_prompt(history,user_name,boot_name):\n",
    "#     prompt = f\"I am your AI companion {boot_name}. You can start a conversation by inputting content, clear clears the conversation history, and stop terminates the program.\"\n",
    "#     for dialog in history:\n",
    "#         query = dialog[0]\n",
    "#         response = dialog[1]\n",
    "#         prompt += f\"\\n\\n{user_name}：{query}\"\n",
    "#         prompt += f\"\\n\\n{boot_name}：{response}\"\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe2d957-d3c3-4b3d-a001-f11e451d180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory_data_and_memory_index(user_name, user_memory_index_path, memory_data, memory_data_dir, input_text, response):\n",
    "    '''\n",
    "    summary: this function updates the memory data and memory index\n",
    "    '''\n",
    "    \n",
    "    def update_memory_data(user_name, memory_data, memory_data_dir, input_text, response):\n",
    "        \n",
    "        class LLMSummarizerClient:\n",
    "            def __init__(self, gen_config=None):\n",
    "                self.summarizer_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))   \n",
    "                self.gen_config = gen_config \n",
    "\n",
    "            def generate_text_simple(self,prompt):              \n",
    "                max_retry_count = 3\n",
    "                cur_retry_count = 0\n",
    "                chat_completion = None\n",
    "                while not chat_completion and cur_retry_count < max_retry_count:\n",
    "                    try:\n",
    "                        request = copy.deepcopy(self.gen_config)\n",
    "                        messages = [\n",
    "                        {\"role\": \"system\", \"content\": \"Below is a transcript of a conversation between a human and an AI assistant that is intelligent and knowledgeable in psychology.\"},\n",
    "                        {\"role\": \"user\", \"content\": \"Hello! Please help me summarize the content of the conversation.\"},\n",
    "                        {\"role\": \"system\", \"content\": \"Sure, I will do my best to assist you.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"{prompt}\"}]\n",
    "                        chat_completion = self.summarizer_client.chat.completions.create(**request, messages=messages)\n",
    "                        # response = openai.ChatCompletion.create(**request, messages=messages)\n",
    "                       \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        if 'This model\\'s maximum context' in str(e):\n",
    "                                cut_length = 1800-200*(count)\n",
    "                                print('max context length reached, cut to {}'.format(cut_length))\n",
    "                                prompt = prompt[-cut_length:]\n",
    "                                response=None\n",
    "                    finally:\n",
    "                        cur_retry_count += 1\n",
    "                if chat_completion:\n",
    "                    # task_desc = response['choices'][0]['message']['content']\n",
    "                    task_desc = chat_completion.choices[0].message.content\n",
    "                else:\n",
    "                    task_desc = ''\n",
    "                return task_desc\n",
    "\n",
    "        chatgpt_config = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 400,\n",
    "                \"top_p\": 1.0,\n",
    "                \"frequency_penalty\": 0.4,\n",
    "                \"presence_penalty\": 0.2, \n",
    "                \"stop\": [\"<|im_end|>\"]\n",
    "                }\n",
    "        \n",
    "        llm_client = LLMSummarizerClient(chatgpt_config)\n",
    "\n",
    "        def summarize_content_prompt(content,user_name,boot_name):\n",
    "            prompt = 'Please summarize the following dialogue as concisely as possible, extracting the main themes and key information. If there are multiple key events, you may summarize them separately. Dialogue content:\\n'\n",
    "            for dialog in content:\n",
    "                query = dialog['query']\n",
    "                response = dialog['response']\n",
    "                prompt += f\"\\n{user_name}：{query.strip()}\"\n",
    "                prompt += f\"\\n{boot_name}：{response.strip()}\"\n",
    "            prompt += ('\\nSummarization：')\n",
    "            return prompt\n",
    "    \n",
    "        def summarize_person_prompt(content,user_name,boot_name):\n",
    "            prompt = f\"Based on the following dialogue, please summarize {user_name}'s personality traits and emotions, and devise response strategies based on your speculation. Dialogue content:\\n\"\n",
    "            for dialog in content:\n",
    "                query = dialog['query']\n",
    "                response = dialog['response']\n",
    "                prompt += f\"\\n{user_name}：{query.strip()}\"\n",
    "                prompt += f\"\\n{boot_name}：{response.strip()}\"\n",
    "        \n",
    "            prompt += (f\"\\n{user_name}'s personality traits, emotions, and {boot_name}'s response strategy are:\")\n",
    "            return prompt\n",
    "        \n",
    "        def summarize_overall_prompt(content):\n",
    "            prompt = \"Please provide a highly concise summary of the following event, capturing the essential key information as succinctly as possible. Summarize the event:\\n\"\n",
    "            for date,summary_dict in content:\n",
    "                summary = summary_dict['content']\n",
    "                prompt += (f\"At {date}, the events are {summary.strip()}\")\n",
    "            prompt += ('\\nSummarization：')\n",
    "            return prompt\n",
    "\n",
    "        def summarize_overall_personality_prompt(content):\n",
    "            prompt = \"The following are the user's exhibited personality traits and emotions throughout multiple dialogues, along with appropriate response strategies for the current situation:\"\n",
    "            for date,summary in content:\n",
    "                prompt += (f\"At {date}, the analysis shows {summary.strip()}\")\n",
    "            prompt += (\"Please provide a highly concise and general summary of the user's personality and the most appropriate response strategy for the AI lover, summarized as:\")\n",
    "            return prompt\n",
    "\n",
    "        \n",
    "        if user_name is None:\n",
    "            return\n",
    "        todays_date = time.strftime(\"%Y-%m-%d\", time.localtime())\n",
    "\n",
    "        if user_name not in memory_data:\n",
    "            memory_data[user_name] = {}\n",
    "            memory_data[user_name]['name'] = user_name\n",
    "    \n",
    "        if memory_data[user_name].get(\"history\") is None:\n",
    "            memory_data[user_name].update({\"history\":{}})\n",
    "        \n",
    "        if memory_data[user_name]['history'].get(todays_date) is None:\n",
    "            memory_data[user_name]['history'][todays_date] = []\n",
    "        \n",
    "        memory_data[user_name]['history'][todays_date].append({'query':input_text,'response':response})\n",
    "        \n",
    "        for stored_name, stored_memory in memory_data.items():\n",
    "            if stored_name != user_name:\n",
    "                continue\n",
    "                \n",
    "            print(f'*** Updating memory for user {user_name} ***')\n",
    "            if \"history\" not in stored_memory:\n",
    "                break\n",
    "            \n",
    "            history = stored_memory[\"history\"]\n",
    "            \n",
    "            if \"summary\" not in stored_memory:\n",
    "                memory_data[user_name][\"summary\"] = {}\n",
    "            if \"personality\" not in stored_memory:\n",
    "                memory_data[user_name][\"personality\"] = {}\n",
    "    \n",
    "            update_overall_summary = False\n",
    "            update_overall_personality = False\n",
    "            \n",
    "            for date, content in history.items():\n",
    "                \n",
    "                # This is a flag to check if the summary was already created for that day and it is not today - dont not create a new one for that day unless its today\n",
    "                create_summary_for_the_date = False if (date != todays_date and date in stored_memory[\"summary\"] and stored_memory[\"summary\"][date]) else True\n",
    "                # This is a flag to check if the peronality was already careated for that day and it is not today - dont create a new one for that day unless its today\n",
    "                create_personality_for_the_date = False if (date != todays_date and date in stored_memory[\"personality\"] and stored_memory[\"personality\"][date]) else True\n",
    "    \n",
    "                summary_prompt = summarize_content_prompt(content,user_name,boot_name)\n",
    "                personality_prompt = summarize_person_prompt(content,user_name,boot_name)\n",
    "                \n",
    "                if create_summary_for_the_date:\n",
    "                    history_summary = llm_client.generate_text_simple(prompt=summary_prompt)\n",
    "                    memory_data[user_name]['summary'][date] = {'content':history_summary}\n",
    "                    update_overall_summary = True\n",
    "                    # print(\"generated summary\", history_summary)\n",
    "                if create_personality_for_the_date:\n",
    "                    personality_summary = llm_client.generate_text_simple(prompt=personality_prompt)\n",
    "                    memory_data[user_name][\"personality\"][date] = personality_summary\n",
    "                    update_overall_personality = True\n",
    "                    # print(\"generated personality\", personality_summary)\n",
    "                    \n",
    "            if update_overall_summary:\n",
    "                overall_summary_prompt = summarize_overall_prompt(list(memory_data[user_name][\"summary\"].items()))\n",
    "            if update_overall_personality:\n",
    "                overall_personality_prompt = summarize_overall_personality_prompt(list(memory_data[user_name][\"personality\"].items()))\n",
    "    \n",
    "            memory_data[user_name][\"overall_history\"] = llm_client.generate_text_simple(prompt=overall_summary_prompt)\n",
    "            # print(\"generated overall history summary: \",  memory_data[user_name][\"overall_history\"] )\n",
    " \n",
    "            memory_data[user_name][\"overall_personality\"] = llm_client.generate_text_simple(prompt=overall_personality_prompt)\n",
    "            # print(\"generated overall personality summary\", memory_data[user_name][\"overall_personality\"])\n",
    "            \n",
    "            json.dump(memory_data,open(memory_data_dir,\"w\",encoding=\"utf-8\"),ensure_ascii=False)\n",
    "\n",
    "    def update_memory_index(user_name, memory_data):\n",
    "\n",
    "        def generate_user_memory_doc(user_name, memory_data):\n",
    "            \n",
    "            user_memory_doc = []\n",
    "            for name, memory in memory_data.items():\n",
    "                if name != user_name:\n",
    "                    continue\n",
    "                # print(f\"generating user memory doc for the user : {name}\")\n",
    "                \n",
    "                if 'history' not in memory.keys():\n",
    "                    continue\n",
    "                memory_arr = []\n",
    "                for date, content in memory['history'].items():\n",
    "                    memory_arr.append(f'Conversation on {date}：')\n",
    "                    for dialog in content:\n",
    "                        query = dialog['query']\n",
    "                        response = dialog['response']\n",
    "                        memory_arr.append(f'\\n{user_name}：{query.strip()}')\n",
    "                        memory_arr.append(f'\\nAI：{response.strip()}')\n",
    "                    memory_arr.append('\\n')\n",
    "                    if 'summary' in memory:\n",
    "                        if date in memory['summary'].keys():\n",
    "                            memory_arr.append(f'The summary of the conversation on {date} is: {memory[\"summary\"][date]}')\n",
    "                    if 'personality' in memory:\n",
    "                        if date in memory['personality'].keys():\n",
    "                            memory_arr.append(f'The personality analysis for the date {date} is: {memory[\"personality\"][date]}')\n",
    "                    memory_str = \"\".join(memory_arr)\n",
    "                    # print(\"memory_str: \", memory_str)\n",
    "                    user_memory_doc = Document(text=memory_str)\n",
    "                    # print(\"user_memory_doc: \", user_memory_doc)\n",
    "                    # print(\"type of user_memory_doc: \", type(user_memory_doc))\n",
    "            return user_memory_doc\n",
    "\n",
    "        \n",
    "        # index_set = {}\n",
    "\n",
    "        user_memory_doc = generate_user_memory_doc(user_name, memory_data)\n",
    "        \n",
    "        # llm_predictor = LLMPredictor(llm=OpenAIChat(model_name=\"gpt-3.5-turbo\"))\n",
    "        # prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "        # service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "        \n",
    "\n",
    "        cur_index = VectorStoreIndex.from_documents([user_memory_doc])\n",
    "        # index_set[user_name] = cur_index\n",
    "        os.makedirs(f'./memory_index/',exist_ok=True)\n",
    "        # cur_index.save_to_disk(f'./memory_index/{user_name}_index.json')\n",
    "        # cur_index.storage_context.persist(persist_dir=f'./memory_index/{user_name}_index.json')\n",
    "        cur_index.storage_context.persist(persist_dir=f'./memory_index/{user_name}_index')\n",
    "        \n",
    "    \n",
    "    # 1) update memory data\n",
    "    update_memory_data(user_name, memory_data, memory_data_dir, input_text, response)\n",
    "    \n",
    "    # 2) update memory index\n",
    "    update_memory_index(user_name, memory_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082c4d6c-dabd-435a-bf3e-00833560602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter Your Name\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User Name： mimi\n"
     ]
    }
   ],
   "source": [
    "print('Please Enter Your Name')\n",
    "user_name = input(\"\\nUser Name：\")\n",
    "\n",
    "# Clear the history and start fresh\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf01903-1f0e-408e-aaf9-966ae0fb4576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello mimi, nice to meet you\n"
     ]
    }
   ],
   "source": [
    "if user_name in memory_data:\n",
    "    user_memory = memory_data[user_name]\n",
    "    user_memory_index = get_user_memory_index(user_name)\n",
    "    print(f\"Welcome back {user_name}\")\n",
    "else: \n",
    "    print(f\"Hello {user_name}, nice to meet you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cde77bb3-fb43-4c89-bb3c-14787c77c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to use LanguageFriend，please enter your question to start conversation, enter \"stop\" to stop program :\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "mimi： I love coding challenges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageFriend : That's great to hear! Coding challenges can be a fun and engaging way to sharpen your programming skills. Is there a specific type of coding challenge you enjoy the most?\n",
      "*** Updating memory for user mimi ***\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "mimi： stop\n"
     ]
    }
   ],
   "source": [
    "print(f\"Welcome to use {boot_actual_name}，please enter your question to start conversation, enter \\\"stop\\\" to stop program :\")\n",
    "while True:\n",
    "    input_text = input(f\"\\n{user_name}：\")\n",
    "\n",
    "    # exit the conversation\n",
    "    if input_text.strip() == \"stop\":\n",
    "                break\n",
    "\n",
    "    # print(\"user_memory_index : \", user_memory_index)\n",
    "    # print(\"all the methods: \", dir(user_memory_index))\n",
    "    # step1: query the memory index if the user's memory_index exists\n",
    "    related_memos = \"\"\n",
    "    if user_memory_index:\n",
    "        related_memos = get_related_memos_from_memory_index(user_memory_index, input_text)\n",
    "    # print(\"rleated_memos : \", related_memos)\n",
    "    \n",
    "    # step2: build the prompt and request gpt api with related_info if it exists and prints the response\n",
    "    result = \"\"\n",
    "    updated_history_to_show_user, updated_history, response = build_prompt_and_ask_gpt(related_memos, user_memory, input_text, history)\n",
    "    history = updated_history\n",
    "\n",
    "    # step3: update the relevant piece of memory_data and memory_index on every interaction\n",
    "    update_memory_data_and_memory_index(user_name, user_memory_index, memory_data, memory_data_dir, input_text, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb7fb9-f4ca-44b5-82b1-cdf537a4cdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
