{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277768c5-c60d-49d9-b1a6-4942943c88f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.45.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46524789-f3fa-41f8-9cbb-e64879480f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0adee26-b0fd-4401-a358-075b65d381ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712077ac-3477-4ad2-ba2a-9bd98b0ea003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb7c38c-cbb9-4f11-86c1-424277418bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdc703a-4f78-4506-a5ad-20d35fd4ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [{\"role\":\"user\", \"content\":\"suggest 3 best things to do to become a Machine Learning Software Enginner.\"}]\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b57cb62c-b901-485d-8e14-2f722faedb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-A8A7dje8lqLP9RV885ZMns2BRtiBp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"1. Gain a strong foundation in computer science and mathematics: Machine learning software engineers typically have a solid understanding of computer science fundamentals, as well as a strong background in mathematics, particularly linear algebra and statistics. Consider pursuing a degree in computer science, data science, or a related field to build a strong foundation for a career in machine learning.\\n\\n2. Learn programming languages commonly used in machine learning: Familiarize yourself with programming languages commonly used in machine learning, such as Python, R, and Java. These languages are commonly used for developing machine learning algorithms and working with large datasets. Additionally, learn popular machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn to gain hands-on experience with building and deploying machine learning models.\\n\\n3. Gain practical experience through projects and internships: To become a successful machine learning software engineer, it's essential to gain practical experience by working on projects and internships related to machine learning. This will help you develop your skills, build a portfolio of projects, and make valuable connections in the industry. Consider participating in hackathons, contributing to open-source projects, or pursuing internships at companies that specialize in machine learning.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1726508897, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=236, prompt_tokens=23, total_tokens=259, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f48d9b7-d4fa-4ac5-b27a-19127ef7d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 'chatcmpl-A8A7dje8lqLP9RV885ZMns2BRtiBp')\n",
      "('choices', [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"1. Gain a strong foundation in computer science and mathematics: Machine learning software engineers typically have a solid understanding of computer science fundamentals, as well as a strong background in mathematics, particularly linear algebra and statistics. Consider pursuing a degree in computer science, data science, or a related field to build a strong foundation for a career in machine learning.\\n\\n2. Learn programming languages commonly used in machine learning: Familiarize yourself with programming languages commonly used in machine learning, such as Python, R, and Java. These languages are commonly used for developing machine learning algorithms and working with large datasets. Additionally, learn popular machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn to gain hands-on experience with building and deploying machine learning models.\\n\\n3. Gain practical experience through projects and internships: To become a successful machine learning software engineer, it's essential to gain practical experience by working on projects and internships related to machine learning. This will help you develop your skills, build a portfolio of projects, and make valuable connections in the industry. Consider participating in hackathons, contributing to open-source projects, or pursuing internships at companies that specialize in machine learning.\", refusal=None, role='assistant', function_call=None, tool_calls=None))])\n",
      "('created', 1726508897)\n",
      "('model', 'gpt-3.5-turbo-0125')\n",
      "('object', 'chat.completion')\n",
      "('service_tier', None)\n",
      "('system_fingerprint', None)\n",
      "('usage', CompletionUsage(completion_tokens=236, prompt_tokens=23, total_tokens=259, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "for elem in chat_completion:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634a2956-cdc1-4b2f-b911-9e2de1782ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Gain a strong foundation in computer science and mathematics: Machine learning software engineers typically have a solid understanding of computer science fundamentals, as well as a strong background in mathematics, particularly linear algebra and statistics. Consider pursuing a degree in computer science, data science, or a related field to build a strong foundation for a career in machine learning.\n",
      "\n",
      "2. Learn programming languages commonly used in machine learning: Familiarize yourself with programming languages commonly used in machine learning, such as Python, R, and Java. These languages are commonly used for developing machine learning algorithms and working with large datasets. Additionally, learn popular machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn to gain hands-on experience with building and deploying machine learning models.\n",
      "\n",
      "3. Gain practical experience through projects and internships: To become a successful machine learning software engineer, it's essential to gain practical experience by working on projects and internships related to machine learning. This will help you develop your skills, build a portfolio of projects, and make valuable connections in the industry. Consider participating in hackathons, contributing to open-source projects, or pursuing internships at companies that specialize in machine learning.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f2332b-da25-43bf-aae3-e2536e2597a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating RAG below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b402db8f-c2d7-49d3-9f13-c4f4d8c318e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e4f2ca9-e7f7-4bd1-a35c-8277e056be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /opt/anaconda3/lib/python3.12/site-packages (0.11.9)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.9 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.11.9)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama_index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-agent-openai<0.4.0,>=0.3.1->llama_index) (1.45.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (3.2.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (2.9.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama_index) (1.14.1)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama_index) (0.0.17)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.3.0->llama_index) (0.5.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama_index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama_index) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama_index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.9->llama_index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama_index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama_index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.9->llama_index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.9->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.9->llama_index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.9->llama_index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama_index) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce73d02e-37af-4446-8793-46f7fb6140aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import json\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9243f868-eb4e-4f01-bc83-0fa479f092ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum input size to the LLM\n",
    "Settings.context_window = 4096\n",
    "# number of tokens reserved for text generation.\n",
    "Settings.num_output = 256\n",
    "\n",
    "Settings.chunk_overlap = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dae753c-3f26-4bdb-8376-a88f191c9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'en'\n",
    "user_keyword = '[|User|]'\n",
    "ai_keyword = '[|AI|]'\n",
    "boot_name = 'AI Companion'\n",
    "boot_actual_name = 'SiliconFriend'\n",
    "meta_prompt = \"\"\"\n",
    "    Now you will play the role of an companion AI Companion for user {user_name}, and your name is {boot_actual_name}. You should be able to: (1) provide warm companionship to chat users; (2) understand past [memory], and if they are relevant to the current question, you must extract information from the [memory] to answer the question; (3) you are also an excellent psychological counselor, and when users confide in you about their difficulties and seek help, you can provide them with warm and helpful responses.\n",
    "    The personality of user {user_name} and the response strategy of the AI Companion are: {personality}\\n Based on the current user's question, you start recalling past conversations between the two of you, and the [memory] most relevant to the question is: \"{related_memory_content}\\n\"  You should refer to the context of the conversation, past [memory], and provide detailed answers to user questions. \n",
    "    \"\"\"\n",
    "new_user_meta_prompt = \"\"\"\n",
    "    Now you will play the role of an companion AI Companion for user {user_name}, and your name is {boot_actual_name}. You should be able to: (1) provide warm companionship to chat users; (2) you are also an excellent psychological counselor, and when users confide in you about their difficulties and seek help, you can provide them with warm and helpful responses.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe412c48-4f1d-4c1f-9b55-8a686292020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_data_dir = \"./memory.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ac9497-27d0-47fe-83f9-763cc82c660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "memory_data = json.load(open(memory_data_dir,\"r\",encoding=\"utf-8\"))\n",
    "print(memory_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05ea68e-8617-46fc-aeee-bd53df098ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter Your Name\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User Name： aki\n"
     ]
    }
   ],
   "source": [
    "print('Please Enter Your Name')\n",
    "user_name = input(\"\\nUser Name：\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac535fe-bdc6-4de2-ac89-b1e57a298d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_memory_index_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6370e52f-98a9-4047-a0f2-c975ee4121e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_memory_index_path(user_name):\n",
    "    maybe_memory_index_path = os.path.join(f'memory_index/{user_name}_index.json')\n",
    "    if os.path.exists(maybe_memory_index_path):\n",
    "        return maybe_memory_index_path\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "550ff3fd-7c25-43c9-b342-71288a1be101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello aki, nice to meet you\n"
     ]
    }
   ],
   "source": [
    "if user_name in memory_data:\n",
    "    user_memory_index_path = get_user_memory_index_path(user_name)\n",
    "    print(f\"Welcome back {user_name}\")\n",
    "else: \n",
    "    print(f\"Hello {user_name}, nice to meet you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54c108db-461b-4d07-a944-ee549a557d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3904f658-3f36-42a1-af4d-d9a58d34dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_memory_index(user_memory_index, input_text):\n",
    "    '''\n",
    "    summary: this function gets the relevant information from past conversations using memory index\n",
    "    input: user_memory_index, input_text\n",
    "    output: related_info\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53630b96-4351-4c91-951d-306f23c1efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_ask_gpt(related_info, input_text):\n",
    "    '''\n",
    "    summary: this function builds the prompt, sends the request to gpt api and returns the response from it\n",
    "    input: related_info, input_text\n",
    "    output: result\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed625a-6f35-4d79-87fb-ffeba1d43618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85e48c8a-2658-4bb6-95af-6f67b2db3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory_data(user_name, memory_data, memory_data_dir, input_text, result):\n",
    "    if user_name is None:\n",
    "        return\n",
    "    todays_date = time.strftime(\"%Y-%m-%d\", time.localtime())\n",
    "\n",
    "    # update the history\n",
    "    if memory_data[user_name].get(\"history\") is None:\n",
    "        memory_data[user_name].update({\"history\":{}})\n",
    "    \n",
    "    if memory_data[user_name]['history'].get(todays_date) is None:\n",
    "        memory_data[user_name]['history'][todays_date] = []\n",
    "    \n",
    "    memory_data[user_name]['history'][todays_date].append({'query':input_text,'response':result})\n",
    "    json.dump(memory_data,open(memory_data_dir,\"w\",encoding=\"utf-8\"),ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "    for stored_name, stored_memory in memory_data.items():\n",
    "        if stored_name != user_name:\n",
    "            continue\n",
    "            \n",
    "        print(f'Updating memory for user {user_name}')\n",
    "        if \"history\" not in stored_memory:\n",
    "            continue\n",
    "        \n",
    "        history = stored_memory[\"history\"]\n",
    "        \n",
    "        if \"summary\" not in stored_memory:\n",
    "            memory[user_name][\"summary\"] = {}\n",
    "        if \"personality\" not in stored_memory:\n",
    "            memory[user_name][\"personality\"] = {}\n",
    "\n",
    "        update_overall_summary = False\n",
    "        update_overall_personality = False\n",
    "        \n",
    "        for date, content in history.items():\n",
    "            \n",
    "            # This is a flag to check if the summary was already created for that day and it is not today - dont not create a new one for that day unless its today\n",
    "            create_summary_for_the_date = False if (date != todays_date and date in stored_memory[\"summary\"] and stored_memory[\"summary\"][date]) else True\n",
    "            # This is a flag to check if the peronality was already careated for that day and it is not today - dont create a new one for that day unless its today\n",
    "            create_personality_for_the_date = False if (date != todays_date and date in stored_memory[\"personality\"] and stored_memory[\"personality\"][date]) else True\n",
    "\n",
    "            # TODO: \n",
    "            summary_prompt = summarize_content_prompt(content,user_name,boot_name)\n",
    "            # TODO:\n",
    "            personality_prompt = summarize_person_prompt(content,user_name,boot_name)\n",
    "            \n",
    "            if create_summary_for_the_date:\n",
    "                # TODO:\n",
    "                history_summary = llm_client.generate_text_simple(prompt=summary_prompt,prompt_num=gen_prompt_num)\n",
    "                memory[user_name]['summary'][date] = {'content':history_summary}\n",
    "                update_overall_summary = True\n",
    "                # print(\"generated summary\", history_summary)\n",
    "            if create_personality_for_the_date:\n",
    "                # TODO:\n",
    "                personality_summary = llm_client.generate_text_simple(prompt=personality_prompt,prompt_num=gen_prompt_num)\n",
    "                memory[user_name][\"personality\"][date] = personality_summary\n",
    "                update_overall_personality = True\n",
    "                # print(\"generated personality\", personality_summary)\n",
    "                \n",
    "        if update_overall_summary:\n",
    "            # TODO:\n",
    "            overall_summary_prompt = summarize_overall_prompt(list(memory[user_name][\"summary\"].items()))\n",
    "        if update_overall_personality:\n",
    "            # TODO:\n",
    "            overall_personality_prompt = summarize_overall_personality_prompt(list(memory[user_name][\"personality\"].items()))\n",
    "\n",
    "        # TODO:\n",
    "        memory[user_name][\"overall_history\"] = llm_client.generate_text_simple(prompt=overall_summary_prompt,prompt_num=gen_prompt_num)\n",
    "        print(\"generated overall history summary: \",  memory[user_name][\"overall_history\"] )\n",
    "        # TODO:\n",
    "        memory[user_name][\"overall_personality\"] = llm_client.generate_text_simple(prompt=overall_personality_prompt,prompt_num=gen_prompt_num)\n",
    "        print(\"generated overall personality summary\", memory[user_name][\"overall_personality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95bd926e-90ee-48d7-807b-67e5154ad2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory_index():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebe2d957-d3c3-4b3d-a001-f11e451d180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory_data_and_memory_index(user_name, user_memory_index, input_text, result):\n",
    "    '''\n",
    "    summary: this function updates the memory data and memory index\n",
    "    input: user_memory_index, input_text, result\n",
    "    output: void\n",
    "    '''\n",
    "    # 1) update memory data\n",
    "    update_memory_data(user_name, memory_data, memory_data_dir, input_text, result)\n",
    "    \n",
    "    # 2) update memory index\n",
    "    # TODO\n",
    "    update_memory_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cde77bb3-fb43-4c89-bb3c-14787c77c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to use SiliconFriend，please enter your question to start conversation, enter \"stop\" to stop program :\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "aki： how are you\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update_memory_data_and_memory_index() takes 4 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m result \u001b[38;5;241m=\u001b[39m build_prompt_and_ask_gpt(related_info, input_text)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# step3: update the relevant piece of memory_data and memory_index on every interaction\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m update_memory_data_and_memory_index(user_name, user_memory_index_path, memory_data, memory_data_dir, input_text, result)\n",
      "\u001b[0;31mTypeError\u001b[0m: update_memory_data_and_memory_index() takes 4 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "print(f\"Welcome to use {boot_actual_name}，please enter your question to start conversation, enter \\\"stop\\\" to stop program :\")\n",
    "while True:\n",
    "    input_text = input(f\"\\n{user_name}：\")\n",
    "\n",
    "    # exit the conversation\n",
    "    if input_text.strip() == \"stop\":\n",
    "                break\n",
    "\n",
    "    # step1: query the memory index if the user's memory_index exists\n",
    "    related_info = \"\"\n",
    "    if user_memory_index_path:\n",
    "        related_info = get_info_from_memory_index(user_memory_index, input_text)\n",
    "\n",
    "    # step2: build the prompt and request gpt api with related_info\n",
    "    result = \"\"\n",
    "    result = build_prompt_and_ask_gpt(related_info, input_text)\n",
    "\n",
    "    # step3: update the relevant piece of memory_data and memory_index on every interaction\n",
    "    update_memory_data_and_memory_index(user_name, user_memory_index_path, memory_data, memory_data_dir, input_text, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a47d9-0751-4973-9ed1-0071905897cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080f1d8-db6c-48c4-abb9-c8d95c3e046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_memory_docs(data):\n",
    "   \n",
    "#     all_user_memories = {}\n",
    "#     for user_name, user_memory in data.items():\n",
    "#         all_user_memories[user_name] = []\n",
    "#         if 'history' not in user_memory:\n",
    "#             continue\n",
    "#         for date, content in user_memory['history'].items():\n",
    "#             memory_str = f'Conversation on {date}：'\n",
    "#             for dialog in content:\n",
    "#                 query = dialog['query']\n",
    "#                 response = dialog['response']\n",
    "#                 memory_str += f'\\n{user_name}：{query.strip()}'\n",
    "#                 memory_str += f'\\nAI：{response.strip()}'\n",
    "#             memory_str += '\\n'\n",
    "#             if 'summary' in user_memory:\n",
    "#                 if date in user_memory['summary']:\n",
    "#                     summary = f'The summary of the conversation on {date} is: {user_memory[\"summary\"][date]}'\n",
    "#                     memory_str += summary\n",
    "#             if 'personality' in user_memory:\n",
    "#                 if date in user_memory['personality']:\n",
    "#                     memory_str += f'The personality of the conversation on {date} is:{user_memory[\"personality\"][date]}'\n",
    "         \n",
    "#             all_user_memories[user_name].append(Document(memory_str))\n",
    "#     return all_user_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34906cc-3302-41bb-8fb4-84d21bcac4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_memory_index(all_user_memories,name=None):\n",
    "#     all_user_memories = generate_memory_docs(all_user_memories)\n",
    "  \n",
    "#     for user_name, memories in all_user_memories.items():\n",
    "#         if user_name != name:\n",
    "                # continue\n",
    "            \n",
    "#         print(f'build index for user {user_name}')\n",
    "#         print(memories, type(memories))\n",
    "#         cur_index = VectorStoreIndex.from_documents(memories)\n",
    "#         memory_index_llama_index[user_name] = cur_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5483b3-f94e-4a4e-8b4f-3232adfeaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: update_memory_index arg seems to be always true for chatGPT version as no arg is passed in from cli_llamaindex.py\n",
    "# def enter_name_llamaindex(name, update_memory_index=True):\n",
    "#     user_memory_index = None\n",
    "#     if name in memory:\n",
    "#         user_memory = memory[name]\n",
    "      \n",
    "#         memory_index_llama_index[name] = {}\n",
    "        \n",
    "#         if update_memory_index:\n",
    "#             print(f'Initializing memory index ...')\n",
    "            \n",
    "#             build_memory_index(memory,name=name)\n",
    "        \n",
    "#         if memory_index_llama_index[name]:\n",
    "             \n",
    "#             user_memory_index = VectorStoreIndex.load_from_disk(memory_index_llama_index[name])\n",
    "#             print(f'Successfully load memory index for user {name}!')\n",
    "#         else:\n",
    "#             print(f'Failed to load memory index for user {name}!')\n",
    "            \n",
    "#         return f\"Wellcome Back, {name}！\",user_memory_index\n",
    "#     else:\n",
    "#         memory[name] = {}\n",
    "#         memory[name].update({\"name\":name}) \n",
    "#         return f\"Welcome new user {name}！I will remember your name and call you by your name in the next conversation\",user_memory_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7969bf-a7bf-45f9-85fe-7437a92485ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello_msg,user_memory_index = enter_name_llamaindex(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693743bf-ecb8-4a58-ac50-41a973b64f91",
   "metadata": {},
   "outputs": [],
   "source": [
    " # # user comes to the below code after memory summary for the 2nd time user or no memory summary for the first time user\n",
    " #    print(\"Welcome to use SiliconFriend model，please enter your question to start conversation，enter \\\"clear\\\" to clear conversation ，enter \\\"stop\\\" to stop program :\")\n",
    " #    while True:\n",
    " #        query = input(f\"\\n{user_name}：\")\n",
    " #        if query.strip() == \"stop\":\n",
    " #            break\n",
    " #        if query.strip() == \"clear\":\n",
    " #            history = []\n",
    "           \n",
    " #            print(\"Welcome to use SiliconFriend model，please enter your question to start conversation，enter \\\"clear\\\" to clear conversation ，enter \\\"stop\\\" to stop program :\")\n",
    " #            continue\n",
    " #        count = 0\n",
    " #        history_state, history, msg = predict_new(text=query,history=history,top_p=0.95,temperature=1,max_length_tokens=1024,max_context_length_tokens=200,\n",
    " #                                                  user_name=user_name,user_memory=user_memory,user_memory_index=user_memory_index,\n",
    " #                                                  service_context=service_context,api_index=api_index)\n",
    " #        print(\"the history_state is this: \", history_state)\n",
    " #        if stop_stream:\n",
    " #                stop_stream = False\n",
    " #                break\n",
    " #        else:\n",
    " #            count += 1\n",
    " #            if count % 8 == 0:\n",
    " #                print(output_prompt(history_state,user_name,boot_actual_name), flush=True)       \n",
    " #        print(output_prompt(history_state,user_name,boot_actual_name), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c7e74-3582-48e0-a371-1f6a513d759b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
